{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-31/blob/main/M3_AST_31_Quantization_%26_Pruning_C%20Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Nwm4FK3wgU"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment : IoT and Edge Devices - Quantization and Pruning of Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu26Vq9jDTpj"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "*  understand about quantization\n",
        "*  batchnorm folding\n",
        "*  quantization aware training\n",
        "*  understand role of pruning in minimization of the resource(power, memory, number of computations) requirements at test time\n",
        "*  implement iterative pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NiLPMhPgYFa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9CxCComgihL"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0k4a9Qbe6p1"
      },
      "source": [
        "In this experiment, we will use the CIFAR-10 dataset from keras API. It consists of 60,000 colour images(32x32) in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images.\n",
        "\n",
        "\n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:\n",
        "\n",
        "\n",
        "<img src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Images/CIFAR10.png\" alt=\"Drawing\" height=\"350\" width=\"440\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "Deep learning has a growing history of successes, but heavy algorithms running on large graphical processing units are far from ideal. A relatively new family of deep learning methods called quantized neural networks have appeared in answer to this discrepancy. Quantization methods helps enabling efficient high-performance deep learning computation on small devices.\n",
        "\n",
        "Moreover, Deep learning for classification tasks involves training the parameters of a neural network such that the algorithm learns to discern between object classes. This is achieved by feeding many images of labelled data to the neural network, while updating the parameters to increase performance on a smooth objective function. A drawback is that a large number of parameters are used, compared to more traditional algorithms.\n",
        "\n",
        "Thus enters quantization as a method to bring the neural network to a reasonable size, while also achieving high performance accuracy. This is especially important for on-device applications, where the memory size and number of computations are necessarily limited. Quantization for deep learning is the process of approximating a neural network that uses floating-point numbers by a neural network of low bit width numbers. This dramatically reduces both the memory requirement and computational cost of using neural networks.\n",
        "\n",
        "\n",
        "**Note:** Refer to the following to understand more about [Quantization](https://pytorch.org/docs/stable/quantization.html)"
      ],
      "metadata": {
        "id": "TFNIzE-djhjR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2237180\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"6366871391\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GXbNUL2L6LoU",
        "outputId": "185c1637-c6be-41f4-ed0f-85980df8c8b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_31_Quantization_&_Pruning_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2237180&recordId=2640\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required packages"
      ],
      "metadata": {
        "id": "pNYuAjidjlGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LuznkOLEjwAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining helper functions to download and load the data"
      ],
      "metadata": {
        "id": "11swMCS8j5V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "\n",
        "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
        "\n",
        "    train_transform = transforms.Compose([   # Define transformations for train and test sets\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    # Loading train and test dataset\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform)\n",
        "\n",
        "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    # A Sampler that randomly shuffled indices\n",
        "    # A RandomSampler with a size and dtype for the stored indices.\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
        "\n",
        "    # A Sampler that returns indices sequentially\n",
        "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
        "\n",
        "    # Loading the train and test dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_set, batch_size=train_batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_set, batch_size=eval_batch_size,\n",
        "        sampler=test_sampler, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "zdcj6vRVvS8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization Code for Model Training"
      ],
      "metadata": {
        "id": "qdJu24TpjqFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source code for torchvision.models.resnet"
      ],
      "metadata": {
        "id": "E7dVMz_YyOLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
        "           'wide_resnet50_2', 'wide_resnet101_2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}"
      ],
      "metadata": {
        "id": "A7kMsVo8x00k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8sXiKScEmcZ"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        # Rename relu to relu1\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "        # Remember to use two independent ReLU for layer fusion.\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # Use FloatFunctional for addition for quantization compatibility\n",
        "        # out += identity\n",
        "        out = self.skip_add.add(identity, out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # out += identity\n",
        "        out = self.skip_add.add(identity, out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # Each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
        "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    arch: str,\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\n",
        "    layers: List[int],\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    **kwargs: Any\n",
        ") -> ResNet:\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
        "                   **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Training Function"
      ],
      "metadata": {
        "id": "48mKTQq3ilZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, device):\n",
        "\n",
        "    # Learning rate\n",
        "    learning_rate = 1e-2\n",
        "\n",
        "    # No. of epochs\n",
        "    num_epochs = 10\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Model Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate through the dataloader\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Model Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        print(\"Epoch: {:02d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QxBD7txEighQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Test Function"
      ],
      "metadata": {
        "id": "o0kT-zi4jEo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "    # Setting model mode to eval()\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterating through the test dataloader\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy"
      ],
      "metadata": {
        "id": "p0tbCRMPGK1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-training quantization (PTQ) can reduce the memory footprint and latency of deep model inference, while still\n",
        "# preserving the accuracy of the model, with only a small unlabeled calibration set and without the retraining on full training set.\n",
        "# To calibrate a quantized model, current PTQ methods usually randomly select some unlabeled data from training set as calibration data.\n",
        "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        _ = model(inputs)\n",
        "\n",
        "# how to measure inference time correctly\n",
        "# measure the time for model inference\n",
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 3, 32, 32),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    # WAIT FOR GPU SYNC\n",
        "    # torch.cuda.synchronize(). This line of code performs synchronization between the host and device (i.e., GPU and CPU),\n",
        "    # so the time recording takes place only after the process running on the GPU is finished.\n",
        "    # Waits for everything to finish running\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad(): # No gradients calculation\n",
        "        # Start time\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        # Inference time took for the model to run the samples on GPU\n",
        "        end_time = time.time()\n",
        "    # Difference between start and end time\n",
        "    elapsed_time = end_time - start_time\n",
        "    # Average elapsed time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave"
      ],
      "metadata": {
        "id": "w7tkSSlGjWkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and load the models"
      ],
      "metadata": {
        "id": "ltA8B4_YqAJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the the model\n",
        "def save_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)\n",
        "\n",
        "# Load the model using pytorch state dict\n",
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Save an offline version of this module for use in a separate process.\n",
        "# The saved module serializes all of the methods, submodules, parameters, and attributes of this module.\n",
        "# https://pytorch.org/docs/stable/generated/torch.jit.save.html\n",
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "\n",
        "# All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from\n",
        "# https://pytorch.org/docs/stable/generated/torch.jit.load.html\n",
        "def load_torchscript_model(model_filepath, device):\n",
        "\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Ozz4IsLFjaxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the model"
      ],
      "metadata": {
        "id": "N5u_C0Q6qtgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model(without pretraining) with no of clasess\n",
        "def create_model(num_classes=10):\n",
        "    model = resnet18(num_classes=num_classes, pretrained=False)\n",
        "    return model\n",
        "\n",
        "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
        "\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        y1 = model_1(x).detach().cpu().numpy()\n",
        "        y2 = model_2(x).detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "3ZFM9Vx1jgDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization\n",
        "\n",
        "* Quantization refers to techniques for doing both computations and memory accesses with lower precision data, usually **int8** compared to floating point implementations.\n",
        "    \n",
        "* Quantization leverages 8bit integer (int8) instructions to reduce the model size and run the inference faster (reduced latency).\n",
        "    \n",
        "* This enables providing quick inference from a trained model and even fitting it into the resources available on a mobile device.\n",
        "\n",
        "* Quantization allows for siginificant performance gains!\n",
        "    * Up to 4x reduction in model size.\n",
        "    * Up to 2-4x reduction in memory bandwidth.\n",
        "    * Up to 2-4x faster inference due to savings in memory bandwidth and faster compute with int8 arithmetic (the exact speed up varies depending on the hardware, the runtime, and the model).\n",
        "    \n",
        "* Quantization doesn't come without additional cost, as it means introducing approximations and the resulting networks have slightly less accuracy.\n",
        "    \n",
        "* These techniques attempt to minimize the gap between the full floating point accuracy and the quantized accuracy.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/d94bdf2f6668d55de89d3c4b602b971e1fb1699c//assets/tut_compress_quant.png\" width=750px/>\n",
        "</center>\n",
        "\n",
        "\n",
        "* Quantization is available in PyTorch in various flavors starting in version 1.3 and there are published quantized models for ResNet, ResNext, MobileNetV2, GoogleNet, InceptionV3 and ShuffleNetV2 in the PyTorch torchvision 0.5 library.\n",
        "\n",
        "* Quantization is compatible with the rest of PyTorch: quantized models are traceable and scriptable. Quantized and floating point operations can be mixed in a model.\n",
        "\n",
        "* Mapping of floating point tensors to quantized tensors is customizable with user defined observer/fake-quantization blocks. PyTorch provides default implementations that should work for most use cases.\n",
        "\n",
        "* Currently the quantized models can only be run on CPU. However, it is possible to send the non-quantized parts of the model to a GPU.\n",
        "    * GPU quantization is a work-in-progress, see PTQ (Post Training Quantization)\n",
        "\n"
      ],
      "metadata": {
        "id": "QFZN0QvZzmOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define a quantized version of our network.\n",
        "\n",
        "For this, apply `torch.quantization.QuantStub()` and `torch.quantization.DequantStub()` to the inputs and outputs, respectively.\n",
        "\n",
        "**Note**:  This step is to ask PyTorch to specifically collect quantization statistics for the inputs and outputs, respectively. Otherwise, since PyTorch collects quantization statistics for weights and activations by default, there will be problems for the input quantization and output dequantization, since there are no quantization statistics collected for them."
      ],
      "metadata": {
        "id": "Fqruslkzb7tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a floating point model where some layers could be statically quantized\n",
        "class QuantizedResNet18(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedResNet18, self).__init__()\n",
        "\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "        # FP32 model\n",
        "        # FP32 is a FP32 Floating point data format for Deep Learning where data is represented as a 32-bit floating point number.\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "T172-SvRcCK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post training static quantization\n",
        "\n",
        "Static quantization first feeds batches of data through the network and computes the resulting distributions of the different activations.\n",
        "Static quantization quantizes the weights and activations of the model. It fuses activations into preceding layers wherever possible. It requires calibration with a representative dataset to determine optimal quantization parameters for activations. Post Training Quantization is typically used when both memory bandwidth and compute savings are important with CNNs being a typical use case. Static quantization is also known as **Post Training Quantization** or **PTQ**.\n",
        "\n",
        "Steps involved in post training static quantization are :\n",
        "1.  Load pretrained model or train a model.\n",
        "2.  Fuse modules - combine operations/modules into a single module to obtain higher accuracy and performance. This is done using the fuse_modules() API, which takes in lists of modules to be fused. We currently support the following fusions: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]\n",
        "3.  Evaluate model in calibration dataset.\n",
        "4.  Calibration data is used to calibrate the model. It is usually a subset of training data.\n",
        "5.  Calculate dynamic ranges of weights and activations in the network to calculate quantization parameters(scale and zero point).\n",
        "6.  Quantize the network using quantization parameters and run the inference.\n",
        "\n",
        "\n",
        "Apply quantization after training, quantization parameters are calculated based on sample calibration data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXC8E1GccL6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 0\n",
        "num_classes = 10\n",
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "model_dir = \"saved_models\"\n",
        "model_filename = \"resnet18_cifar10.pt\"\n",
        "quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
        "model_filepath = os.path.join(model_dir, model_filename)\n",
        "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
        "\n",
        "set_random_seeds(random_seed=random_seed)\n",
        "\n",
        "# Create an untrained model.\n",
        "model = create_model(num_classes=num_classes)\n",
        "\n",
        "train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
        "\n",
        "# Train model.\n",
        "model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device)\n",
        "\n",
        "# Save model.\n",
        "save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
        "\n",
        "# Load a pretrained model.\n",
        "model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
        "\n",
        "# Move the model to CPU since static quantization does not support CUDA currently.\n",
        "model.to(cpu_device)\n",
        "\n",
        "# Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# The model has to be switched to evaluation mode before any layer fusion.\n",
        "# Otherwise the quantization will not work correctly.\n",
        "fused_model.eval()"
      ],
      "metadata": {
        "id": "VxW2caO-Fe_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusing batchnorm with convolution saves extra computation at time of inference. It is also known as `Batchnorm folding`**.\n",
        "\n",
        "We update the weight and bias  of convolution layer before batchnorm to simulate the effect of batchnorm.\n",
        "\n",
        "Equations (5) & (6) denote the same  \n",
        "\n",
        "<br>\n",
        "\n",
        "![bn_folding.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "ygkiYBoQ8sVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fuse the model in place rather manually.\n",
        "# Fuse the activations to preceding layers, where applicable.\n",
        "# This needs to be done manually depending on the model architecture.\n",
        "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
        "# https://pytorch.org/docs/stable/generated/torch.quantization.fuse_modules.html\n",
        "# It returns model with fused modules. A new copy is created if inplace=True.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "for module_name, module in fused_model.named_children():\n",
        "    if \"layer\" in module_name:\n",
        "        for basic_block_name, basic_block in module.named_children():\n",
        "            torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "            for sub_block_name, sub_block in basic_block.named_children():\n",
        "                if sub_block_name == \"downsample\":\n",
        "                    torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
        "\n",
        "# Print FP32 model.\n",
        "print(model)\n",
        "\n",
        "# Print fused model.\n",
        "print(fused_model)\n",
        "\n",
        "# Model and fused model should be equivalent.\n",
        "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\""
      ],
      "metadata": {
        "id": "EgzFQyjY-Hwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Converting networks to use both integer arithmetic and int8 memory accesses can improve the latency performance.\n",
        "    \n",
        "* Static quantization first feeds batches of data through the network and computes the resulting distributions of the different activations.\n",
        "    * This is done by inserting observer modules at different points that record these distributions.\n",
        "    \n",
        "* This information is used to determine how specifically the different activations should be quantized at inference time.\n",
        "        \n",
        "* A simple technique would be to simply divide the entire range of activations into 256 levels, but PyTorch supports more sophisticated methods as well.\n",
        "    \n",
        "* This step allows to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.\n",
        "    \n",
        "* **Optimizing static quantization includes:**\n",
        "    * **Observers:** observer modules specify how statistics are collected prior to quantization to try out more advanced methods to quantize the data.\n",
        "\n",
        "    * **Operator fusion:** fuse multiple operations into a single operation, saving on memory access while also improving the operations numerical accuracy.\n",
        "    \n",
        "    * **Per-channel quantization:** we can independently quantize weights for each output channel in a convolution/linear layer, which can lead to higher accuracy with almost the same speed.\n"
      ],
      "metadata": {
        "id": "53Hwkm3J2WDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will perform the following:\n",
        "\n",
        "*  Specify quantization configurations, such as symmetric quantization or asymmetric quantization, etc.\n",
        "*  Prepare quantization model for post-training calibration.\n",
        "*  Run post-training calibration.\n",
        "*  Convert the calibrated floating point model to quantized integer model.\n",
        "*  Verify accuracies and inference performance gain.\n",
        "*  Save the quantized integer model."
      ],
      "metadata": {
        "id": "QstO3rQocSXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for static quantization. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "\n",
        "# Using un-fused model will fail.\n",
        "# Because there is no quantized layer implementation for a single batch normalization layer.\n",
        "# quantized_model = QuantizedResNet18(model_fp32=model) # This will not work\n",
        "# Select quantization schemes from\n",
        "# https://pytorch.org/docs/stable/quantization-support.html\n",
        "# When preparing a quantized model, it is necessary to ensure that qconfig and the engine used for\n",
        "# quantized computations match the backend on which the model will be executed. The qconfig controls the type of\n",
        "# observers used during the quantization passes. The qengine controls whether fbgemm or qnnpack specific packing\n",
        "# function is used when packing weights for linear and convolution functions and modules.\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "\n",
        "# Custom quantization configurations\n",
        "# quantization_config = torch.quantization.default_qconfig\n",
        "# quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
        "\n",
        "quantized_model.qconfig = quantization_config\n",
        "\n",
        "# Print quantization configurations\n",
        "print(quantized_model.qconfig)\n",
        "\n",
        "torch.quantization.prepare(quantized_model, inplace=True)\n",
        "\n",
        "# Use training data for calibration.\n",
        "calibrate_model(model=quantized_model, loader=train_loader, device=cpu_device)\n",
        "\n",
        "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "\n",
        "# Using high-level static quantization wrapper\n",
        "# The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
        "# quantized_model = torch.quantization.quantize(model=quantized_model, run_fn=calibrate_model, run_args=[train_loader], mapping=None, inplace=False)\n",
        "\n",
        "quantized_model.eval()\n",
        "\n",
        "# Print the quantized model.\n",
        "print(quantized_model)\n",
        "\n",
        "# Save the quantized model.\n",
        "save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "# Load the quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "# Skip this assertion since the values might deviate a lot.\n",
        "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
        "\n",
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "id": "_LrHNHHuHXZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization aware training\n",
        "Static quantization allows the user to generate quantized integer model that is highly efficient during inference. However, sometimes, even with careful post-training calibration, the model accuracies might be sacrificed to some extent that is not acceptable. If this is the case, post-training calibration is not sufficient to generate a quantized integer model. We would have train the model in a way so that the quantization effect has been taken into account. Quantization aware training is capable of modeling the quantization effect during training.\n",
        "\n",
        "The mechanism of quantization aware training is simple, it places fake quantization modules, i.e., all weights and activations are fake quantized during both the forward and backward passes of training: quantization and dequantization modules, at the places where quantization happens during floating-point model to quantized integer model conversion, to simulate the effects of clamping and rounding brought by integer quantization. The fake quantization modules will also monitor scales and zero points of the weights and activations. Once the quantization aware training is finished, the floating point model could be converted to quantized integer model immediately using the information stored in the fake quantization modules."
      ],
      "metadata": {
        "id": "B9LMCuRRNIZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_qat(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=5):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
        "\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    # Evaluation\n",
        "    # model.eval()\n",
        "    # eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "    # print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(-1, eval_loss, eval_accuracy))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        if epoch > 3:\n",
        "        # Freeze quantizer parameters\n",
        "          model.apply(torch.quantization.disable_observer)\n",
        "\n",
        "        if epoch > 2:\n",
        "        # Freeze batch norm mean and variance estimates\n",
        "          model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "OpOUfWFT7npm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model_filename = \"resnet18_QAT_quantized_cifar10.pt\"\n",
        "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
        "\n",
        "set_random_seeds(random_seed=random_seed)\n",
        "\n",
        "# Create an untrained model.\n",
        "model = create_model(num_classes=num_classes)\n",
        "\n",
        "train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
        "\n",
        "# # Load a pretrained model.\n",
        "model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
        "\n",
        "# Move the model to CPU since static quantization does not support CUDA currently.\n",
        "model.to(cpu_device)\n",
        "\n",
        "# Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)\n",
        "\n",
        "# Model and fused model should be equivalent.\n",
        "model.eval()\n",
        "fused_model.eval()\n",
        "\n",
        "\n",
        "# Fuse the model in place rather manually.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "for module_name, module in fused_model.named_children():\n",
        "    if \"layer\" in module_name:\n",
        "        for basic_block_name, basic_block in module.named_children():\n",
        "            torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "            for sub_block_name, sub_block in basic_block.named_children():\n",
        "                if sub_block_name == \"downsample\":\n",
        "                    torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
        "\n",
        "# Print FP32 model.\n",
        "print(model)\n",
        "\n",
        "# Print fused model.\n",
        "print(fused_model)\n",
        "\n",
        "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\""
      ],
      "metadata": {
        "id": "0U8gTtVnHYlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the model for quantization aware training."
      ],
      "metadata": {
        "id": "Gsd4vE8m3IT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for quantization aware training. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "\n",
        "# Using un-fused model will fail.\n",
        "# Because there is no quantized layer implementation for a single batch normalization layer.\n",
        "# quantized_model = QuantizedResNet18(model_fp32=model)\n",
        "# Select quantization schemes from\n",
        "# https://pytorch.org/docs/stable/quantization-support.html\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "\n",
        "# Custom quantization configurations\n",
        "# quantization_config = torch.quantization.default_qconfig\n",
        "# quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
        "\n",
        "quantized_model.qconfig = quantization_config\n",
        "\n",
        "# Print quantization configurations\n",
        "print(quantized_model.qconfig)\n",
        "\n",
        "# https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#prepare_qat\n",
        "torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
        "\n",
        "# # Use training data for calibration.\n",
        "print(\"Training QAT Model...\")\n",
        "quantized_model.train()\n",
        "\n",
        "train_model_qat(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=5)\n",
        "quantized_model.to(cpu_device)\n",
        "\n",
        "# Using high-level static quantization wrapper\n",
        "# The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
        "# quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
        "\n",
        "# Convert the observed model to a quantized model. This does several things:\n",
        "# quantizes the weights, computes and stores the scale and bias value to be\n",
        "# used with each activation tensor, fuses modules where appropriate,\n",
        "# and replaces key operators with quantized implementations.\n",
        "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "quantized_model.eval()\n",
        "\n",
        "# Print quantized model.\n",
        "print(quantized_model)\n",
        "\n",
        "# Save quantized model.\n",
        "save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "# Load quantized model.\n",
        "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "# Skip this assertion since the values might deviate a lot.\n",
        "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "print(\"QAT FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"QAT INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
        "\n",
        "print(\"QAT FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "print(\"QAT FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "print(\"QAT INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "print(\"QAT INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "id": "7HwZae8GJ8KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruning using PyTorch"
      ],
      "metadata": {
        "id": "XvWpgt4JjV4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning models these days require a significant amount of computing, memory, and power which becomes a bottleneck in the conditions where we need real-time inference or to run models on edge devices and browsers with limited computational resources. Energy efficiency is a major concern for current deep learning models. One of the methods for tackling this efficiency is enabling inference efficiency.\n",
        "\n",
        "**Larger Model => More Memory References => More Energy**\n",
        "\n",
        "Pruning is one of the methods for inference to efficiently produce models smaller in size, more memory-efficient, more power-efficient and faster at inference with minimal loss in accuracy, other such techniques being weight sharing and quantization. Out of several aspects that deep learning takes as an inspiration from the area of Neuroscience. Moreover, pruning in deep learning is also a biologically inspired concept. Now, let us begin with it's implementation.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/720/1*nicFUkeUpWMW1w_hUVtZiw.png\" width=650px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "L0Hg1PF-jVRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required libraries"
      ],
      "metadata": {
        "id": "teKufNMwi3Fw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNk2Xgu6UTG3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import copy\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX_iW6N-sLkF"
      },
      "source": [
        "### Effects of Pruning Neural Nets\n",
        "\n",
        "\n",
        "\n",
        "Pruning is deleting connections in a neural net in order to improve generalization and reduce computational resources.\n",
        "Two most common type of pruning are :\n",
        "\n",
        "1.   Weight-pruning\n",
        "2.   Unit-pruning\n",
        "\n",
        "In **weight pruning**  the largest weights by absolute value are set to zero.\n",
        "While in **unit-pruning** , the smallest neurons are set to zero by a vector-wise metric like **L2-norm**.\n",
        "\n",
        "Here, we examine the relationship between pruning and accuracy on a vanilla neural net. Before running any experiments, we hypothesize that accuracy for the pruned neural net will slightly rise (due to the regularization), and then have a negative linear correlation with the amount pruned. we also hypothesize that unit-pruning, in deleting entire neurons instead of individual weights, will have a more dramatic negative effect than weight-pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZmejOzw8MHS"
      },
      "source": [
        "First, let's load, normalize, and visualize the MNIST dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5QgQR6UUlwF"
      },
      "source": [
        "def load_MNIST():\n",
        "  \"\"\"Function to load and normalize MNIST data\"\"\"\n",
        "  train = torchvision.datasets.MNIST(root='./data', download=True, train=True, transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                                ]))\n",
        "  test = torchvision.datasets.MNIST(root='./data', download=True, train=False, transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                                ]))\n",
        "  print(\"MNIST datset loaded and normalized.\")\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train, shuffle=True, batch_size=100)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=test, shuffle=False, batch_size=100)\n",
        "  print(\"PyTorch DataLoaders loaded.\")\n",
        "  return train, test, train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X3rzpxCXv4R"
      },
      "source": [
        "def visualize_MNIST(train_loader):\n",
        "  \"\"\"Function to visualize data given a DataLoader object\"\"\"\n",
        "  dataiter = iter(train_loader)\n",
        "  images, labels = next(dataiter)\n",
        "  print(\"image shape:\", images.shape, \"\\n label shape:\", labels.shape)\n",
        "  # visualize data\n",
        "  fig, ax = plt.subplots(2,5)\n",
        "  for i, ax in enumerate(ax.flatten()):\n",
        "      im_idx = np.argwhere(labels == i)[0][0]\n",
        "      plottable_image = images[im_idx].squeeze()\n",
        "      ax.imshow(plottable_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awglncFta6ga"
      },
      "source": [
        "# load and visualize MNIST\n",
        "train, test, train_loader, test_loader = load_MNIST()\n",
        "visualize_MNIST(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avoio0W48Yzi"
      },
      "source": [
        "### Build Vanilla Neural Network\n",
        "\n",
        "Now let's build a vanilla neural net with four hidden layers without pruning.\n",
        "\n",
        "We'll keep things simple and leave out biases, convolutions, and pooling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnUxUg7iU4yn"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  \"\"\"A non-sparse neural network with four hidden fully-connected layers\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.input_layer = nn.Linear(784, 1000, bias=False)\n",
        "    self.hidden1_layer = nn.Linear(1000, 1000, bias=False)\n",
        "    self.hidden2_layer = nn.Linear(1000, 500, bias=False)\n",
        "    self.hidden3_layer = nn.Linear(500, 200, bias=False)\n",
        "    self.hidden4_layer = nn.Linear(200, 10, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden1_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden2_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden3_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden4_layer(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqA-fHwd8toX"
      },
      "source": [
        "### Model Training\n",
        "\n",
        "Let's train our vanilla neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJcwD5-NeJBS"
      },
      "source": [
        "def train(model, train_loader, epochs=3, learning_rate=0.001):\n",
        "  \"\"\"Function to train a neural net\"\"\"\n",
        "\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  time0 = time()\n",
        "  total_samples = 0\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(\"Starting epoch\", e)\n",
        "    total_loss = 0\n",
        "\n",
        "    for idx, (images,labels) in enumerate(train_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      optimizer.zero_grad() # zero out the gradients\n",
        "      output = model(images) # forward pass\n",
        "      loss = lossFunction(output,labels) # calculate loss\n",
        "      loss.backward() # backpropagate\n",
        "      optimizer.step() # update weights\n",
        "\n",
        "      total_samples += labels.size(0)\n",
        "      # Compute the loss\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if idx % 100 == 0:\n",
        "        print(\"Running loss:\", total_loss)\n",
        "\n",
        "  final_time = (time()-time0)/60\n",
        "  print(\"Model trained in \", final_time, \"minutes on \", total_samples, \"samples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51kd-weohlPf"
      },
      "source": [
        "model = Net()\n",
        "train(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArL0CsYL83AB"
      },
      "source": [
        "### Model Testing\n",
        "\n",
        "Now we'll test our vanilla neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u61CAZip1UNx"
      },
      "source": [
        "def test(model, test_loader):\n",
        "  \"\"\"Test neural net\"\"\"\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, (images, labels) in enumerate(test_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      output = model(images) # Forward pass\n",
        "      values, indices = torch.max(output.data, 1) # maximum probability of the predictions\n",
        "      total += labels.size(0)\n",
        "      correct += (labels == indices).sum().item()\n",
        "\n",
        "    # Accuracy of model\n",
        "    acc = correct / total * 100\n",
        "    # print(\"Accuracy: \", acc, \"% for \", total, \"training samples\")\n",
        "\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAz77644j08"
      },
      "source": [
        "acc = test(model, test_loader)\n",
        "print(\"The accuracy of our vanilla NN is\", acc, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8QEw_mWGA3D"
      },
      "source": [
        "A ~96% accuracy for our vanilla neural network seems reasonable. Now let's do some weight and unit pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function for weight pruning"
      ],
      "metadata": {
        "id": "mao97ZAki8H2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3EavCDY4mOg"
      },
      "source": [
        "def sparsify_by_weights(model, k):\n",
        "\n",
        "  \"\"\"Function that takes un-sparsified neural net and does weight-pruning\n",
        "  by k sparsity\"\"\"\n",
        "\n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, i in enumerate(sparse_m.parameters()):\n",
        "      if idx == 4: # skip last layer of 5-layer neural net\n",
        "        break\n",
        "      # change tensor to numpy format, then set appropriate number of smallest weights to zero\n",
        "      layer_copy = torch.flatten(i)\n",
        "      layer_copy = layer_copy.detach().numpy()\n",
        "      indices = abs(layer_copy).argsort() # get indices of smallest weights by absolute value\n",
        "      indices = indices[:int(len(indices)*k)] # get k fraction of smallest indices\n",
        "      layer_copy[indices] = 0\n",
        "\n",
        "      # change weights of model\n",
        "      i = torch.from_numpy(layer_copy)\n",
        "\n",
        "  return sparse_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function for unit pruning"
      ],
      "metadata": {
        "id": "B1-8zJjojErP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTb5ptg80hqV"
      },
      "source": [
        "def l2(array):\n",
        "  return np.sqrt(np.sum([i**2 for i in array]))\n",
        "\n",
        "def sparsify_by_unit(model, k):\n",
        "  \"\"\"Creates a k-sparsity model with unit-level pruning that sets columns with smallest L2 to zero.\"\"\"\n",
        "\n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "\n",
        "  for idx, i in enumerate(sparse_m.parameters()):\n",
        "    if idx == 4: # skip last layer of 5-layer neural net\n",
        "      break\n",
        "    layer_copy = i.detach().numpy()\n",
        "    indices = np.argsort([l2(i) for i in layer_copy])\n",
        "    indices = indices[:int(len(indices)*k)]\n",
        "    layer_copy[indices,:] = 0\n",
        "    i = torch.from_numpy(layer_copy)\n",
        "\n",
        "  return sparse_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX21TSoq3udv"
      },
      "source": [
        "def get_pruning_accuracies(model, prune_type):\n",
        "  \"\"\" Takes a model and prune type (\"weight\" or \"unit\") and returns a DataFrame of pruning accuracies for given sparsities.\"\"\"\n",
        "\n",
        "  df = pd.DataFrame({\"sparsity\": [], \"accuracy\": []})\n",
        "  sparsities = [0.0, 0.25, 0.50, 0.60, 0.70, 0.80]#, 0.90, 0.95, 0.97, 0.99]\n",
        "\n",
        "  for s in sparsities:\n",
        "    if prune_type == \"weight\":\n",
        "      new_model = sparsify_by_weights(model, s)\n",
        "    elif prune_type == \"unit\":\n",
        "      new_model = sparsify_by_unit(model, s)\n",
        "    else:\n",
        "      print(\"Must specify prune-type.\")\n",
        "      return\n",
        "    acc = test(new_model, test_loader)\n",
        "    df = df.append({\"sparsity\": s, \"accuracy\": acc}, ignore_index=True)\n",
        "\n",
        "  return df,new_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIWGnpQZHYSi"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WgshrNX5Is2"
      },
      "source": [
        "df_weight,weight_pruned_model = get_pruning_accuracies(model, \"weight\")\n",
        "df_unit,unit_pruned_model = get_pruning_accuracies(model, \"unit\")\n",
        "\n",
        "print(\"Accuracies for Weight Pruning\")\n",
        "print(df_weight)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Accuracies for Unit Pruning\")\n",
        "print(df_unit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's visualize weights and compare it with unpruned model."
      ],
      "metadata": {
        "id": "XHCjIOdNOdio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_weights(weights, n_weights):\n",
        "\n",
        "    rows = int(np.sqrt(n_weights))\n",
        "    cols = int(np.sqrt(n_weights))\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    for i in range(rows*cols):\n",
        "        ax = fig.add_subplot(rows, cols, i+1)\n",
        "        ax.imshow(weights[i].view(28, 28).cpu().numpy(), cmap='bone')\n",
        "        ax.axis('off')\n",
        "N_WEIGHTS = 9\n",
        "\n",
        "weights = model.input_layer.weight.data\n",
        "\n",
        "plot_weights(weights, N_WEIGHTS)"
      ],
      "metadata": {
        "id": "N9I-ciGKLXMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets visualize weights after pruning\n",
        "# Weight pruning at 80 percent sparsity\n",
        "N_WEIGHTS = 9\n",
        "\n",
        "weights = weight_pruned_model.input_layer.weight.data\n",
        "\n",
        "plot_weights(weights, N_WEIGHTS)"
      ],
      "metadata": {
        "id": "bNpmr_LVNY_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets visualize weights after pruning\n",
        "\n",
        "# Unit pruning at 80 percent sparsity\n",
        "N_WEIGHTS = 9\n",
        "\n",
        "weights = unit_pruned_model.input_layer.weight.data\n",
        "\n",
        "plot_weights(weights, N_WEIGHTS)"
      ],
      "metadata": {
        "id": "JTTzx1kQN-Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVBWjZMMDglS"
      },
      "source": [
        "plt.figure()\n",
        "plt.title(\"Accuracy vs Sparsity\")\n",
        "plt.plot(df_unit[\"sparsity\"], df_unit[\"accuracy\"], label=\"Unit-pruning\")\n",
        "plt.plot(df_weight[\"sparsity\"], df_weight[\"accuracy\"], label=\"Weight-pruning\")\n",
        "plt.xlabel(\"Sparsity (as fraction)\")\n",
        "plt.ylabel(\"% Accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterative Pruning"
      ],
      "metadata": {
        "id": "v0o38Rn74CfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike One-time  pruning + fine-tuning which achieves the desired prune rate by pruning and fine-tuning once, multi-time iterative pruning + fine-tuning achieves the desired prune rate by pruning and fine-tuning multiple-times.\n",
        "\n",
        "For example, to achieve the desired prune rate of\n",
        "98\n",
        "%\n",
        ", we could run pruning and fine-tuning for many iterations, achieving prune rate of\n",
        "30\n",
        "%\n",
        ",\n",
        "50\n",
        "%\n",
        ",\n",
        "66\n",
        "%\n",
        ",\n",
        "76\n",
        "%\n",
        ",\n",
        "\n",
        ",\n",
        "98\n",
        "%\n",
        " in each iteration.\n",
        "\n",
        " So basically we **train for some epochs, prune slightly and train again**. We repeat this process until desired sparsity is reached. In this case we use masks (either 0 or 1 ) per layer to get remember the indices of pruned elements."
      ],
      "metadata": {
        "id": "EeLX9LKIJ9wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLinear(nn.Linear):\n",
        "\t\"\"\" same as Linear except has a configurable mask on the weights \"\"\"\n",
        "\n",
        "\tdef __init__(self, in_features, out_features, bias=True):\n",
        "\t\tsuper().__init__(in_features, out_features, bias)\n",
        "\t\tself.register_buffer('mask', torch.ones(out_features, in_features))\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\treturn F.linear(input, self.mask * self.weight, self.bias)\n",
        "\n",
        "class MaskNet(nn.Module):\n",
        "  \"\"\"A non-sparse neural network with four hidden fully-connected layers\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MaskNet,self).__init__()\n",
        "    self.input_layer = MaskedLinear(784, 1000, bias=False)\n",
        "    self.hidden1_layer = MaskedLinear(1000, 1000, bias=False)\n",
        "    self.hidden2_layer = MaskedLinear(1000, 500, bias=False)\n",
        "    self.hidden3_layer = MaskedLinear(500, 200, bias=False)\n",
        "    self.hidden4_layer = MaskedLinear(200, 10, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden1_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden2_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden3_layer(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.hidden4_layer(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "NDNO5rNOO_0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_model = MaskNet()\n",
        "train(masked_model, train_loader)"
      ],
      "metadata": {
        "id": "ssVHc3r3Rd6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = test(masked_model, test_loader)\n",
        "print(\"The accuracy of our masked vanilla NN is\", acc, \"%\")"
      ],
      "metadata": {
        "id": "CFyraaMvSHIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_prune_rate(max_prune_rate=0.5,start_epoch=0,num_prune_epochs=8,total_epoch=30):\n",
        "    \"\"\"Function to calculate pruning ratio for different pruning epochs\"\"\"\n",
        "    final_prune_epoch = int(0.8*total_epoch) # change ratio to change final prune epoch\n",
        "    #num_prune_epochs = 8\n",
        "    prune_rates = [max_prune_rate*(1 - (1 - (i / num_prune_epochs))**3)\n",
        "                    for i in range(num_prune_epochs)]\n",
        "    prune_rates[-1] = max_prune_rate\n",
        "    prune_epochs = np.linspace(start_epoch, final_prune_epoch, num_prune_epochs).astype('i').tolist()\n",
        "\n",
        "    return prune_rates,prune_epochs\n",
        "\n",
        "def prune_iterative(model, prune_percentage):\n",
        "  \"\"\"Function that takes un-sparsified neural net and does weight-pruning\n",
        "  by k sparsity\"\"\"\n",
        "\n",
        "  # make copy of original neural net\n",
        "  sparse_m = copy.deepcopy(model)\n",
        "  linear_layers = [m for m in sparse_m.modules() if isinstance(m,MaskedLinear) ]\n",
        "  with torch.no_grad():\n",
        "    for idx,layer in enumerate(linear_layers):\n",
        "      if idx == 4: # skip last layer of 5-layer neural net\n",
        "        break\n",
        "      # change tensor to numpy format, then set appropriate number of smallest weights to zero\n",
        "      layer_mask = torch.ones(len(layer.weight.reshape(-1)))\n",
        "      layer_copy = torch.flatten(layer.weight)\n",
        "      layer_copy = layer_copy.detach().numpy()\n",
        "      indices = abs(layer_copy).argsort() # get indices of smallest weights by absolute value\n",
        "      indices = indices[:int(len(indices)*prune_percentage)] # get k fraction of smallest indices\n",
        "      layer_mask[indices] = 0\n",
        "\n",
        "      # change masks of the layer\n",
        "      layer.mask = layer_mask.reshape(layer.weight.shape)\n",
        "\n",
        "\n",
        "  return sparse_m\n",
        "\n",
        "def train_iterative_prune(model, train_loader, epochs=10,learning_rate=0.001):\n",
        "  \"\"\"Function to train and prune  neural network \"\"\"\n",
        "\n",
        "  lossFunction = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  time0 = time()\n",
        "  total_samples = 0\n",
        "  t=0\n",
        "  prune_rates,prune_epochs = gen_prune_rate(max_prune_rate=0.98,start_epoch=0,num_prune_epochs=4,total_epoch=epochs)\n",
        "  print('Pruning rate: ',prune_rates,'Pruning epochs: ',prune_epochs)\n",
        "  for e in range(epochs):\n",
        "    print(\"Starting epoch\", e)\n",
        "    total_loss = 0\n",
        "    if e in prune_epochs:\n",
        "      model = prune_iterative(model,prune_rates[t])\n",
        "\n",
        "      acc = test(model, test_loader)\n",
        "      print(\"Pruning epoch : \",t,\"The accuracy after pruning is\", acc, \"%\")\n",
        "      t+=1\n",
        "\n",
        "    for idx, (images,labels) in enumerate(train_loader):\n",
        "      images = images.view(images.shape[0],-1) # flatten\n",
        "      optimizer.zero_grad() # forward pass\n",
        "      output = model(images)\n",
        "      loss = lossFunction(output,labels) # calculate loss\n",
        "      loss.backward() # backpropagate\n",
        "      optimizer.step() # update weights\n",
        "\n",
        "      total_samples += labels.size(0)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if idx % 100 == 0:\n",
        "        print(\"Running loss:\", total_loss)\n",
        "\n",
        "  final_time = (time()-time0)/60\n",
        "  if e == epochs-1 :\n",
        "    acc = test(model, test_loader)\n",
        "    print(\"Final accuracy  is\", acc, \"%\")\n",
        "  print(\"Model trained in \", final_time, \"minutes on \", total_samples, \"samples\")"
      ],
      "metadata": {
        "id": "HpIgDPacJ85O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterative_prune(masked_model, train_loader)"
      ],
      "metadata": {
        "id": "5F9YGsjHgUsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNGO5Ih9Hv52"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Clearly, my hypothesis that accuracy will rise and then negatively correlate in a roughly linear way with pruning was incorrect. The figure instead shows a dramatic nonlinear relationship between accuracy and pruning. Accuracy remains roughly constant until dropping off at about 75% sparsity for weight-pruning and until 70% sparsity for unit-pruning. My hypothesis that unit-pruning impacts accuracy more dramatically than weight-pruning held up.\n",
        "\n",
        "These results are fascinating: Less than 25% of the neural net represents important information about its function. The data also suggest that accuracy may slightly increase with a light amount of pruning (~30%), although I would run on more iterations with a larger dataset to be sure. It would make sense that keeping the net's smaller weights reduces its generalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Consider the below statements about pruning and answer question Q1:\n",
        "\n",
        "A. In network pruning, we prune the small-weight connections: all connections with weights below a threshold are removed\n",
        "from the network.\n",
        "\n",
        "B. In network pruning, we prune the small-weight connections: all connections with weights above a threshold are removed\n",
        "from the network.\n",
        "\n",
        "C. Pruning is one of the methods for inference to efficiently produce models smaller in size, more memory-efficient, more power-efficient and faster at inference with minimal loss in accuracy, other such techniques being weight sharing and quantization.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7fitFsxdfU4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHDYadgG-RHl",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Which of the above statement(s) is/are true?\n",
        "Answer1 = \"Both A and C\" #@param [\"\",\"Both A and B\", \"Both B and C\", \"Both A and C\", \"Only A\", \"Only B\", \"Only C\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Consider the below statements about quantization and answer question Q2:\n",
        "\n",
        "A. Neural network quantization is a process of reducing the precision of the weights in the neural network, thus reducing the memory, computation, and energy bandwidths.\n",
        "\n",
        "B. When deploying neural networks models on mobile or edge devices, quantization and model compression in general, is desirable and often the only reasonable way to deploy a mobile model because the memory and computational budget of these devices is very limited.\n",
        "\n"
      ],
      "metadata": {
        "id": "-Dtz-Nlq1nz6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu3sufDD7pu1",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. Select the true statement(s) from the above:\n",
        "Answer2 = \"Both A and B\" #@param [\"\",\"Only A\",\"Only B\",\"Both A and B\",\"Neither A nor B\"]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "outputId": "2b08ce81-8a0a-446c-bf2f-2df56d60cd14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2640\n",
            "Date of submission:  09 Sep 2023\n",
            "Time of submission:  14:56:45\n",
            "View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}